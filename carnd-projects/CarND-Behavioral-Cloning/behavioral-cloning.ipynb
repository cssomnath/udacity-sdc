{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.misc\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataHelper:\n",
    "    def __init__(self, data_file):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        \n",
    "        with open(data_file) as f:\n",
    "            header = f.readline()\n",
    "            dirname = os.path.dirname(data_file)\n",
    "            for line in f:\n",
    "                fields = line.split(\", \")\n",
    "                xs.append(os.path.join(dirname, fields[0]))\n",
    "                ys.append(fields[3])\n",
    "                \n",
    "        c = list(zip(xs, ys))\n",
    "        random.shuffle(c)\n",
    "        xs, ys = zip(*c)\n",
    "        \n",
    "        self._batch_pointer = 0\n",
    "        self._train_xs = xs[:(int)(len(xs) * 0.9)]\n",
    "        self._train_ys = ys[:(int)(len(xs) * 0.9)]\n",
    "\n",
    "        val_xs = []\n",
    "        val_ys = []\n",
    "        val_size = (int)(len(xs) * 0.1)\n",
    "        for i in range(val_size):\n",
    "            val_ys.append(ys[-i])\n",
    "            val_xs.append(scipy.misc.imread(xs[-i]))\n",
    "        \n",
    "        self._val_xs = np.asarray(val_xs)\n",
    "        self._val_ys = np.asarray(val_ys)\n",
    "        \n",
    "    def data_size(self):\n",
    "        return len(self._train_ys) + len(self._val_ys)\n",
    "    \n",
    "    def val_data(self):\n",
    "        return self._val_xs, self._val_ys\n",
    "\n",
    "    def next_train_batch(self, batch_size):\n",
    "        x_out = []\n",
    "        y_out = []\n",
    "        for i in range(batch_size):\n",
    "            data_idx = (self._batch_pointer + i) % len(self._train_ys)\n",
    "            y_out.append(self._train_ys[data_idx])\n",
    "            x_out.append(scipy.misc.imread(self._train_xs[data_idx]))\n",
    "\n",
    "        self._batch_pointer += batch_size\n",
    "        return np.asarray(x_out), np.asarray(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8035\n"
     ]
    }
   ],
   "source": [
    "dh = DataHelper('data/driving_log.csv')\n",
    "print(dh.data_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 160, 320, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "x, y = dh.next_train_batch(64)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_conv():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(24, 5, 5, subsample=(2, 2), input_shape=(160, 320, 3),\n",
    "                            activation='relu'))\n",
    "    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\n",
    "    \n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu'))\n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1164, activation='relu'))    \n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_6 (Convolution2D)  (None, 78, 158, 24)   1824        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 37, 77, 36)    21636       convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 17, 37, 48)    43248       convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 13, 33, 64)    76864       convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 9, 29, 64)     102464      convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 16704)         0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 1164)          19444620    flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 100)           116500      dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 50)            5050        dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 10)            510         dense_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 1)             11          dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 19,812,727\n",
      "Trainable params: 19,812,727\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = simple_conv()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Steps 0 train loss 0.969635 validation loss 1.024164 time taken 3.2s\n",
      "Steps 1 train loss 1.035814 validation loss 1.024164 time taken 1.5s\n",
      "Steps 2 train loss 1.002649 validation loss 1.024164 time taken 1.4s\n",
      "Steps 3 train loss 1.057691 validation loss 1.024164 time taken 1.4s\n",
      "Steps 4 train loss 0.998042 validation loss 1.024164 time taken 1.3s\n",
      "Steps 5 train loss 0.992412 validation loss 1.024164 time taken 1.4s\n",
      "Steps 6 train loss 1.052887 validation loss 1.024164 time taken 1.3s\n",
      "Steps 7 train loss 1.040333 validation loss 1.024164 time taken 1.4s\n",
      "Steps 8 train loss 1.075663 validation loss 1.024164 time taken 1.3s\n",
      "Steps 9 train loss 1.004695 validation loss 1.024164 time taken 1.3s\n",
      "Steps 10 train loss 1.065208 validation loss 1.024164 time taken 1.3s\n",
      "Steps 11 train loss 1.021016 validation loss 1.024164 time taken 1.3s\n",
      "Steps 12 train loss 1.026931 validation loss 1.024164 time taken 1.3s\n",
      "Steps 13 train loss 1.031902 validation loss 1.024164 time taken 1.4s\n",
      "Steps 14 train loss 0.992965 validation loss 1.024164 time taken 1.3s\n",
      "Steps 15 train loss 0.981134 validation loss 1.024164 time taken 1.3s\n",
      "Steps 16 train loss 0.993092 validation loss 1.024164 time taken 1.2s\n",
      "Steps 17 train loss 1.052645 validation loss 1.024164 time taken 1.3s\n",
      "Steps 18 train loss 0.989990 validation loss 1.024164 time taken 1.3s\n",
      "Steps 19 train loss 1.007327 validation loss 1.024164 time taken 1.3s\n",
      "Steps 20 train loss 1.021325 validation loss 1.024164 time taken 1.3s\n",
      "Steps 21 train loss 1.000921 validation loss 1.024164 time taken 1.4s\n",
      "Steps 22 train loss 1.018733 validation loss 1.024164 time taken 1.3s\n",
      "Steps 23 train loss 1.047818 validation loss 1.024164 time taken 1.3s\n",
      "Steps 24 train loss 1.022918 validation loss 1.024164 time taken 1.3s\n",
      "Steps 25 train loss 1.071836 validation loss 1.024164 time taken 1.4s\n",
      "Steps 26 train loss 1.044165 validation loss 1.024164 time taken 1.3s\n",
      "Steps 27 train loss 1.062765 validation loss 1.024164 time taken 1.4s\n",
      "Steps 28 train loss 1.069869 validation loss 1.024164 time taken 1.3s\n",
      "Steps 29 train loss 1.079278 validation loss 1.024164 time taken 1.4s\n",
      "Steps 30 train loss 1.036036 validation loss 1.024164 time taken 1.3s\n",
      "Steps 31 train loss 0.980531 validation loss 1.024164 time taken 1.3s\n",
      "Steps 32 train loss 1.099764 validation loss 1.024164 time taken 1.3s\n",
      "Steps 33 train loss 0.947596 validation loss 1.024164 time taken 1.4s\n",
      "Steps 34 train loss 1.016819 validation loss 1.024164 time taken 1.3s\n",
      "Steps 35 train loss 1.008740 validation loss 1.024164 time taken 1.4s\n",
      "Steps 36 train loss 0.983844 validation loss 1.024164 time taken 1.3s\n",
      "Steps 37 train loss 0.948677 validation loss 1.024164 time taken 1.3s\n",
      "Steps 38 train loss 1.009884 validation loss 1.024164 time taken 1.3s\n",
      "Steps 39 train loss 0.997367 validation loss 1.024164 time taken 1.3s\n",
      "Steps 40 train loss 0.976375 validation loss 1.024164 time taken 1.2s\n",
      "Steps 41 train loss 1.051511 validation loss 1.024164 time taken 1.3s\n",
      "Steps 42 train loss 1.060996 validation loss 1.024164 time taken 1.3s\n",
      "Steps 43 train loss 1.038965 validation loss 1.024164 time taken 1.4s\n",
      "Steps 44 train loss 0.979195 validation loss 1.024164 time taken 1.3s\n",
      "Steps 45 train loss 1.016377 validation loss 1.024164 time taken 1.3s\n",
      "Steps 46 train loss 1.065272 validation loss 1.024164 time taken 1.3s\n",
      "Steps 47 train loss 1.002997 validation loss 1.024164 time taken 1.3s\n",
      "Steps 48 train loss 1.057220 validation loss 1.024164 time taken 1.3s\n",
      "Steps 49 train loss 1.055715 validation loss 1.024164 time taken 1.3s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "DATA_SIZE = dh.data_size()\n",
    "\n",
    "val_x, val_y = dh.val_data()\n",
    "\n",
    "print(\"Starting training\")\n",
    "for steps in range(50):\n",
    "    step_start = time.time()\n",
    "    cur_epoch = steps / DATA_SIZE\n",
    "    x, y = dh.next_train_batch(BATCH_SIZE)\n",
    "\n",
    "    train_loss = model.train_on_batch(x, y)\n",
    "    val_loss = model.evaluate(val_x, val_y, verbose=0)\n",
    "    \n",
    "    time_taken = time.time() - step_start\n",
    "    print(\"Steps {} train loss {:0.6f} validation loss {:0.6f} time taken {:0.1f}s\".format(\n",
    "            steps, train_loss, val_loss, time_taken))\n",
    "    \n",
    "    model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "['0' '-0.3445879' '-0.2876218' '0' '0.3583844' '0.1765823' '0' '0' '0'\n",
      " '-0.05975719']\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(val_x[:10])\n",
    "print(y_pred.reshape(10))\n",
    "print(val_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

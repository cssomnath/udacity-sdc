{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def normalize_image(img):\n",
    "    means = np.mean(img, axis=(0, 1))\n",
    "    means = means[None,:]\n",
    "    \n",
    "    std = np.std(img, axis=(0, 1))\n",
    "    std = std[None,:]\n",
    "    return (img - means) / std\n",
    "\n",
    "def preprocess_image(img):\n",
    "    img_crop = img[56:150, :, :]\n",
    "    img_normed = normalize_image(img_crop)\n",
    "    return img_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Class to load data and generate batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataHelper:\n",
    "    def __init__(self, data_file):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        \n",
    "        with open(data_file) as f:\n",
    "            header = f.readline()\n",
    "            dirname = os.path.dirname(data_file)\n",
    "            for line in f:\n",
    "                fields = line.split(\", \")\n",
    "                # Loading only the center images\n",
    "                xs.append(os.path.join(dirname, fields[0]))\n",
    "                ys.append(fields[3])\n",
    "                \n",
    "        c = list(zip(xs, ys))\n",
    "        random.shuffle(c)\n",
    "        xs, ys = zip(*c)\n",
    "        \n",
    "        self._batch_pointer = 0\n",
    "        self._train_xs = xs[:(int)(len(xs) * 0.9)]\n",
    "        self._train_ys = ys[:(int)(len(xs) * 0.9)]\n",
    "\n",
    "        val_xs = []\n",
    "        val_ys = []\n",
    "        val_size = (int)(len(xs) * 0.1)\n",
    "        for i in range(val_size):\n",
    "            img = plt.imread(xs[-i])\n",
    "            img_pre = preprocess_image(img)\n",
    "            \n",
    "            val_xs.append(img_pre)\n",
    "            val_ys.append(ys[-i])\n",
    "        \n",
    "        self._val_xs = np.asarray(val_xs)\n",
    "        self._val_ys = np.asarray(val_ys)\n",
    "        \n",
    "    def data_size(self):\n",
    "        return len(self._train_ys) + len(self._val_ys)\n",
    "    \n",
    "    def val_data(self):\n",
    "        return self._val_xs, self._val_ys\n",
    "\n",
    "    def next_train_batch(self, batch_size):\n",
    "        x_out = []\n",
    "        y_out = []\n",
    "        for i in range(batch_size):\n",
    "            data_idx = (self._batch_pointer + i) % len(self._train_ys)\n",
    "            \n",
    "            img = plt.imread(self._train_xs[data_idx])\n",
    "            img_pre = preprocess_image(img)\n",
    "            \n",
    "            x_out.append(img_pre)            \n",
    "            y_out.append(self._train_ys[data_idx])\n",
    "\n",
    "        self._batch_pointer += batch_size\n",
    "        return np.asarray(x_out), np.asarray(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dh = DataHelper('data/driving_log.csv')\n",
    "print(dh.data_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = dh.next_train_batch(64)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(y) - 1)\n",
    "img = x[idx]\n",
    "print(img.shape)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_conv():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(24, 5, 5, subsample=(2, 2), input_shape=(94, 320, 3),\n",
    "                            activation='relu', init='he_normal'))\n",
    "    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu', init='he_normal'))\n",
    "    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu', init='he_normal'))\n",
    "    \n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu', init='he_normal'))\n",
    "    model.add(Convolution2D(64, 5, 5, activation='relu', init='he_normal'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1164, activation='relu', init='he_normal'))    \n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Dense(100, activation='relu', init='he_normal'))\n",
    "    model.add(Dense(50, activation='relu', init='he_normal'))\n",
    "    model.add(Dense(10, activation='relu', init='he_normal'))\n",
    "    model.add(Dense(1, activation='tanh', init='he_normal'))\n",
    "\n",
    "    opt = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = simple_conv()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_layer = model.layers[-1].get_weights()\n",
    "second_last_layer = model.layers[-2].get_weights()\n",
    "\n",
    "print(\"Second last layer\")\n",
    "print(second_last_layer[0][:2])\n",
    "print(\"output\")\n",
    "print(second_last_layer[1])\n",
    "\n",
    "print(\"Last layer\")\n",
    "print(last_layer[0].reshape(10))\n",
    "print(\"final output\")\n",
    "print(last_layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Steps 0 train loss 0.031671 validation loss 0.016157 time taken 1.2s\n",
      "Steps 1 train loss 0.022991 validation loss 0.016154 time taken 1.3s\n",
      "Steps 2 train loss 0.021220 validation loss 0.016148 time taken 1.2s\n",
      "Steps 3 train loss 0.012573 validation loss 0.016141 time taken 1.1s\n",
      "Steps 4 train loss 0.038619 validation loss 0.016135 time taken 1.1s\n",
      "Steps 5 train loss 0.024697 validation loss 0.016133 time taken 1.1s\n",
      "Steps 6 train loss 0.025924 validation loss 0.016130 time taken 1.1s\n",
      "Steps 7 train loss 0.016732 validation loss 0.016123 time taken 1.1s\n",
      "Steps 8 train loss 0.017975 validation loss 0.016114 time taken 1.2s\n",
      "Steps 9 train loss 0.020340 validation loss 0.016104 time taken 1.2s\n",
      "Steps 10 train loss 0.017827 validation loss 0.016094 time taken 1.2s\n",
      "Steps 11 train loss 0.015686 validation loss 0.016083 time taken 1.1s\n",
      "Steps 12 train loss 0.034869 validation loss 0.016075 time taken 1.2s\n",
      "Steps 13 train loss 0.037945 validation loss 0.016069 time taken 1.1s\n",
      "Steps 14 train loss 0.019101 validation loss 0.016061 time taken 1.2s\n",
      "Steps 15 train loss 0.015280 validation loss 0.016055 time taken 1.1s\n",
      "Steps 16 train loss 0.017308 validation loss 0.016048 time taken 1.2s\n",
      "Steps 17 train loss 0.025146 validation loss 0.016039 time taken 1.2s\n",
      "Steps 18 train loss 0.015661 validation loss 0.016029 time taken 1.3s\n",
      "Steps 19 train loss 0.016917 validation loss 0.016018 time taken 1.2s\n",
      "Steps 20 train loss 0.021259 validation loss 0.016006 time taken 1.2s\n",
      "Steps 21 train loss 0.024852 validation loss 0.015995 time taken 1.2s\n",
      "Steps 22 train loss 0.019817 validation loss 0.015985 time taken 1.2s\n",
      "Steps 23 train loss 0.020502 validation loss 0.015973 time taken 1.2s\n",
      "Steps 24 train loss 0.016755 validation loss 0.015964 time taken 1.2s\n",
      "Steps 25 train loss 0.017656 validation loss 0.015956 time taken 1.2s\n",
      "Steps 26 train loss 0.014347 validation loss 0.015947 time taken 1.1s\n",
      "Steps 27 train loss 0.021658 validation loss 0.015940 time taken 1.2s\n",
      "Steps 28 train loss 0.014451 validation loss 0.015934 time taken 1.2s\n",
      "Steps 29 train loss 0.014635 validation loss 0.015929 time taken 1.2s\n",
      "Steps 30 train loss 0.016843 validation loss 0.015924 time taken 1.2s\n",
      "Steps 31 train loss 0.013151 validation loss 0.015921 time taken 1.2s\n",
      "Steps 32 train loss 0.014680 validation loss 0.015920 time taken 1.2s\n",
      "Steps 33 train loss 0.021622 validation loss 0.015917 time taken 1.2s\n",
      "Steps 34 train loss 0.025860 validation loss 0.015915 time taken 1.2s\n",
      "Steps 35 train loss 0.030012 validation loss 0.015914 time taken 1.2s\n",
      "Steps 36 train loss 0.012521 validation loss 0.015912 time taken 1.2s\n",
      "Steps 37 train loss 0.022895 validation loss 0.015911 time taken 1.2s\n",
      "Steps 38 train loss 0.022283 validation loss 0.015910 time taken 1.2s\n",
      "Steps 39 train loss 0.014660 validation loss 0.015908 time taken 1.2s\n",
      "Steps 40 train loss 0.018257 validation loss 0.015905 time taken 1.2s\n",
      "Steps 41 train loss 0.022051 validation loss 0.015898 time taken 1.1s\n",
      "Steps 42 train loss 0.014947 validation loss 0.015894 time taken 1.1s\n",
      "Steps 43 train loss 0.013723 validation loss 0.015892 time taken 1.1s\n",
      "Steps 44 train loss 0.014382 validation loss 0.015890 time taken 1.2s\n",
      "Steps 45 train loss 0.024847 validation loss 0.015890 time taken 1.2s\n",
      "Steps 46 train loss 0.021844 validation loss 0.015888 time taken 1.1s\n",
      "Steps 47 train loss 0.021157 validation loss 0.015886 time taken 1.2s\n",
      "Steps 48 train loss 0.008148 validation loss 0.015887 time taken 1.2s\n",
      "Steps 49 train loss 0.020602 validation loss 0.015889 time taken 1.1s\n",
      "Steps 50 train loss 0.023407 validation loss 0.015891 time taken 1.2s\n",
      "Steps 51 train loss 0.013492 validation loss 0.015893 time taken 1.2s\n",
      "Steps 52 train loss 0.016048 validation loss 0.015894 time taken 1.2s\n",
      "Steps 53 train loss 0.019507 validation loss 0.015895 time taken 1.1s\n",
      "Steps 54 train loss 0.017614 validation loss 0.015894 time taken 1.2s\n",
      "Steps 55 train loss 0.010804 validation loss 0.015892 time taken 1.1s\n",
      "Steps 56 train loss 0.009640 validation loss 0.015890 time taken 1.2s\n",
      "Steps 57 train loss 0.026925 validation loss 0.015885 time taken 1.2s\n",
      "Steps 58 train loss 0.016438 validation loss 0.015880 time taken 1.1s\n",
      "Steps 59 train loss 0.021267 validation loss 0.015876 time taken 1.2s\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "DATA_SIZE = dh.data_size()\n",
    "\n",
    "val_x, val_y = dh.val_data()\n",
    "\n",
    "print(\"Starting training\")\n",
    "for steps in range(60):\n",
    "    step_start = time.time()\n",
    "    cur_epoch = steps / DATA_SIZE\n",
    "    x, y = dh.next_train_batch(BATCH_SIZE)\n",
    "\n",
    "    train_loss = model.train_on_batch(x, y)\n",
    "    val_loss = model.evaluate(val_x, val_y, verbose=0)\n",
    "    \n",
    "    time_taken = time.time() - step_start\n",
    "    print(\"Steps {} train loss {:0.6f} validation loss {:0.6f} time taken {:0.1f}s\".format(\n",
    "            steps, train_loss, val_loss, time_taken))\n",
    "    \n",
    "    model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01934147 -0.00740436  0.00159991  0.0039825   0.01777484  0.0039825\n",
      "  0.00078883 -0.00402864  0.00298586  0.0039825 ]\n",
      "['0.100034' '-0.09773462' '0' '-0.2021725' '0.0904655' '0' '0' '-0.0787459'\n",
      " '0' '0']\n"
     ]
    }
   ],
   "source": [
    "test_idx = np.random.randint(0, len(val_y), size=10)\n",
    "y_pred = model.predict(val_x[test_idx, ])\n",
    "print(y_pred.reshape(10))\n",
    "print(val_y[test_idx,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second last layer\n",
      "[[ 0.26945588 -0.05191147 -0.3705292  -0.26706398  0.10041998  0.1699838\n",
      "   0.1535826  -0.16913702  0.08579066  0.2918222 ]\n",
      " [-0.13610914 -0.16321753 -0.13842998  0.07048266  0.25611222 -0.04282611\n",
      "   0.02490786  0.19878256  0.17138635  0.05657545]]\n",
      "output\n",
      "[-0.00069582 -0.00348687 -0.00511459 -0.00334825 -0.00389479 -0.00470362\n",
      " -0.00225571 -0.00228998 -0.00367564 -0.00364226]\n",
      "Last layer\n",
      "[ 0.11915189  0.60039246 -0.2546992   0.53063667 -0.46422622 -0.06258549\n",
      "  0.25730845 -0.20006031 -1.58154202  0.3335906 ]\n",
      "final output\n",
      "[ 0.00398253]\n"
     ]
    }
   ],
   "source": [
    "last_layer = model.layers[-1].get_weights()\n",
    "second_last_layer = model.layers[-2].get_weights()\n",
    "\n",
    "print(\"Second last layer\")\n",
    "print(second_last_layer[0][:2])\n",
    "print(\"output\")\n",
    "print(second_last_layer[1])\n",
    "\n",
    "print(\"Last layer\")\n",
    "print(last_layer[0].reshape(10))\n",
    "print(\"final output\")\n",
    "print(last_layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
